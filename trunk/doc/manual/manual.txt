= VizStack Manual =
Hewlett Packard <shreekumar@hp.com>
v0.5, Sep 2009

== Introduction ==

Most commodity servers and workstations shipping today are capable of hosting 
multiple Graphics Processing Units(GPUs). Such machines are typically clustered 
together.

VizStack is a software stack that turns one or more machines(with GPUs installed 
in them) into a shared, multi-user visualization resource.  VizStack provides 
utilities to allocate resources (GPUs), run applications on them, and free them 
when they are no longer needed.  VizStack provides ways to configure and drive 
display devices, as well as higher level constructs like Tiled Displays. 

Using VizStack, you may run as many remote visualization sessions as you have GPUs.
VizStack supports HP's Remote Graphics Software and the TurboVNC/VirtualGL as 
remote visualization solutions. You may use nVidia graphics cards and QuadroPlexes 
to drive display devices. When the GPUs are not being used for display, they 
could be used for to service remote users. If you are a user of ParaView or 
Avizo, you will be able to run these applications directly. 

VizStack manages only the visualization resources (GPUs, X servers), and does 
not provide any utilities to do system management/setup on the nodes. VizStack 
can dynamically setup the visualization resources to meet any application 
requirement.

For ease of use, VizStack provides integrations with Hewlett Packard's Remote 
Graphics Software and TurboVNC/VirtualGL, as well as popular visualization 
applications.

=== Modes of Operation ===

VizStack can be used in three modes

1. Sea of Nodes mode: In this mode, VizStack manages the visualization 
resources on two or more nodes. One of these nodes needs to run VizStack's 
System State Manager (SSM) daemon process. The node running the SSM may or 
may not have GPUs in it.  To run VizStack in this mode, you typically need 
to configure an "application launcher". You have a choice of using these
application launchers:
  a. SLURM, an open source resource manager
  b. SSH, with nodes setup for password-less SSH.
  c. Local "exec". This allows users to allocate and use GPUs on the node 
     from where their jobs get scheduled.  If you're looking to use VizStack
     in conjuction with a scheduler, then you may use either this application 
     launcher, or SSH.
In the sea-of-nodes configuration, you will also need to install & configure 
Munge on all the nodes.

2. Single Node mode: In this mode, VizStack manages the visualization 
resources on exactly one physical node.  Using this mode instantly converts 
a single node into a shareable visualization resource. You may also use the
 node to drive a large display wall or any other display system. This is 
also the easiest way to try out VizStack. If you choose to run VizStack in 
this mode, then you don't need to configure Munge and SLURM.

3. Static Configuration: This mode allows one to setup a static configuration
 of X servers, GPUs, display devices. This mode will be described in later 
versions of this document.

Typically, you may also want to use a remote visualization solution with 
VizStack. VizStack provides direct integration with two software solutions 
for remote visualization:

1. Hewlett-Packard Remote Graphics Software. This provides one remote 
visualization session per physical node.

2. TurboVNC with VirtualGL for running OpenGL apps. You may have one remote 
visualization session per GPU.

You will need to manually install and configure the remote visualization 
solutions of your choice. Note that these solutions are independent of each 
other, and you can use one or both of them at the same time.

== Installing VizStack ==

Before you can install and use VizStack, you need to install the OS which will 
form the underlying environment first. VizStack additionally depends on the 
availability of a few additional software packages (over and above) the OS. 

Note that VizStack does not provide for node management, golden imaging, 
installing Infiniband, configuring IP addresses, etc. VizStack can manage the 
visualization resources on one or more nodes. Before installing VizStack on a 
group of nodes, you need to ensure that Linux is installed and running on each
and every node.

Refer to <<C2_1>> to ensure that your system(s) meet the minimum hardware and 
software requirements.

The steps needed to get VizStack up and running are with your application are

1. For every node
   a. Do kernel setup if needed[<<C2_2>>]
   b. Install graphics drivers, software dependencies needed for VizStack, and 
      the VizStack RPM [<<C2_3>>]

2. Configure required software if running on more than one node
   a. NTP [<<REF_NTP>>]
   b. Munge [<<REF_MUNGE>>]
   c. SLURM [<<REF_SLURM>>] or Password-less SSH [<<REF_SSH>>]

3. Install & configure additional software
   a. RGS or TurboVNC/VirtualGL for remote graphics
   b. Application software like ParaView, Avizo, Ensight, and any other software
      you need.

[[C2_1]]
=== Supported Platforms ===

VizStack is fairly generic software, and should run on most x86 machines with 
Linux installed on them.  The machines need to be equipped with nVidia graphics
 cards or QuadroPlexes footnote:[VizStack will not work with other non-nVidia graphics cards as of September 2009].

VizStack has the following minimum software requirements

1. Xorg 6.9

2. nVidia driver version series 169 or higher

At this time(September 2009), VizStack has been tested with the following Linux distributions

1. RedHat Enterprise Linux (RHEL) 5.x x86_64

2. Novell SuSE Linux Enterprise Server (SLES) 10.2 x86_64

VizStack is being tested on many graphics capable servers, workstations and blade 
workstations made by Hewlett-Packard.

VizStack is typically tested with the the following nVidia graphics options:

1. High End: Quadro FX 5800, 5600 and 4600. QuadroPlex 2200 D2, QuadroPlex 2100 S4, QuadroPlex 2100 M4

2. Medium Range: Quadro FX 3800, 3700 and 3500

3. Entry Range: Quadro FX 1800, 1700 and 1500

[[C2_2]]
=== Kernel Setup on Each Node ===

This is applicable if you are using SLES, or any other distribution which defaults 
to usage of a non-text mode console.  During installation, SLES configures the kernel 
to use a VGA mode instead of a text mode for the virtual consoles.  Usage of the video 
mode can cause screen corruptions and crashes when X servers start and stop.  To avoid 
this, we recommend that you turn off usage of any kind of VGA modes by the kernel.

You can do this by editing /boot/grub/menu.lst and removing any "vga=" options in the file.

[[C2_3]]
=== Installing VizStack and Dependent Software ===

VizStack is distributed as a .tar.gz file. One file is provided for each supported platform,
currently limited to RHEL 5, and SLES 10 SP2.  Each file includes the VizStack RPM, this
document, as well as any software packages that VizStack needs.

****
If you have an earlier version of VizStack installed, then be sure to remove it from all 
the nodes using "rpm -e vizstack".  At this time, you cannot use the upgrade option with 
the VizStack RPM.
****

You will need to install the VizStack RPM, as well as all dependencies on all the nodes 
which you want to use with VizStack.  Note that, if you want to use VizStack in a 
standalone configuration, then you only need to install the VizStack RPM (and its 
dependencies) on one node. 

[NOTE]
============================================================================
The VizStack RPM includes files in /etc/profile.d . These files set
up the environment for proper excecution of the administrative tools as well
as the user scripts. After you install the VizStack RPM, you will need to 
*logout* and log back in again before you run any other commands.
============================================================================

==== Installing nVidia Graphics Drivers ====

First, download the nVidia graphics drivers relevant to your operating system and hardware 
from http://www.nvidia.com/drivers/. You will typically choose "Linux 64 bit" as the Operating
System, and the select the product type/series corresponding to the graphics options you have
 installed.

If you intend to run CUDA programs then you will need to install the driver needed for the 
version of CUDA of your choice. Be sure to check that it is compatible with the graphics 
hardware you have installed.

The driver can be installed on one node by running a single command. E.g,, if you downloaded 
version 177.70.35, then you would run :
----
# sh NVIDIA-Linux-x86_64-177.70.35-pkg2.run
----

IMPORTANT: You must stop all X servers before trying to install the driver.  Switching to run-level 3 is recommended.

==== Installing Software Dependencies ====

VizStack depends on the following packages

1. Xerces-C version 2.x, tested with version 2.7 and 2.8
2. python-xml. If you are running SuSE Enterprise Linux 10 SP2, then you may use the YaST2 tool to install 
   this. This package is typically installed on most other linux distributions.
3. If you need to run VizStack on more than one node
   a. Munge (http://home.gna.org/munge/) for user authentication
   b. Optional : SLURM (https://computing.llnl.gov/linux/slurm/) for application launcher

Munge and SLURM will typically need to be compiled from source. Packages for Xerces-C and python-xml are 
available for most linux distributions.

=== Configuring Required Software ===

Depending on the mode in which you want to run VizStack (and the kind of applications 
you want to use), you will need to configure some of the installed software. Please 
follow the steps to install/configure the required software.

[[REF_NTP]]
==== Network Time Protocol (NTP) ====

Time needs to be in sync on all the nodes that run VizStack. For this, you may need to configure 
each node to use an NTP server.

[[REF_MUNGE]]
==== Munge ====

Munge is used by VizStack to identify user processes. You need to install Munge if you
plan to use VizStack across two or more nodes.

Munge needs a secret key to work. To generate the key
----
# dd if=/dev/urandom bs=1 count=1024 > /etc/munge/munge.key
----

The above command finishes very quickly, but generates a pseudo-random key.  If security 
is a concern, then consider using the below command line (Note: this will take a long time
 to finish!)

----
# dd if=/dev/random bs=1 count=1024 > /etc/munge/munge.key
----

Next, you need to propagate this secret key to all the nodes of the cluster. You may 
use scp for this purpose, as in the example below. The example assumes a cluster with 
5 nodes named as node1, node2,..., node5.
----
# for node in node1 node2 node3 node4 node5; do
	scp /etc/munge/munge.key root@$node:/etc/munge;
  done;
----

Finally, restart the Munge daemon (on all the nodes)
----
# service munge restart
----

[[REF_SLURM]]
==== SLURM ====

If you want to use VizStack on two or more nodes, then you may choose to use the 
SLURM scheduler as an application launch mechanism. Alternatively, you may use 
password-less SSH too. If you prefer to use password-less SSH, then you can skip 
configuring SLURM.  

Using SLURM makes it easier to dynamically administer a few nodes of the system 
without affecting the users who may be using the remaining nodes. Also, SLURM 
configuration is done once and works for all users. If you prefer password-less 
SSH, then bear in mind that the password-less SSH setup may need to be done 
individually for each user.

First, create a user (and group) named 'slurm' on all the nodes, ensuring 
that this user has the same user-id and group-id on all nodes.
----		
# for ((i = 1; i <= 5; i++)); do 
    ssh root@node$i /usr/sbin/groupadd -g 666 slurm;
    ssh root@node$i /usr/sbin/useradd -g 666 -u 666 -M -s /sbin/nologin slurm;
done
----		
In this case, the user will have a uid and gid of 666.

Next, create the JobCredentialPrivateKey file on the master node
----
# openssl genrsa -out /etc/slurm/slurm.key 1024
----
and create the JobCredentialPublicCertificate too
----
# openssl rsa -in /etc/slurm/slurm.key -pubout -out /etc/slurm/slurm.cert
----

Make a copy of the slurm example file from /etc/slurm/slurm.conf.example and 
edit it in your favorite editor. Assuming that your nodes are named node1, 
node2,...,node5 and the node named node1 is the master/control/head node, your 
file needs to look like
----
ControlMachine=node1
...
SlurmUser=slurm
 ...
JobCredentialPrivateKey=/etc/slurm/slurm.key
JobCredentialPublicCertificate=/etc/slurm/slurm.cert
...
NodeName=node[1-5] State=UNKNOWN
PartitionName=viz Nodes=node[1-5] Default=YES RootOnly=NO Shared=FORCE 
MaxTime=INFINITE State=UP
----

After making these changes, copy the slurm.conf, slurm.key and slurm.cert
files to all the nodes.
----
# for ((i = 2; i <= 5; i++)); do 
    scp /etc/slurm/slurm.conf /etc/slurm/slurm.key /etc/slurm/slurm.cert 
        root@node$i:/etc/slurm; 
  done
----

Finally, restart the slurm daemon on all the nodes.
----
# for ((i = 1; i <= 5; i++)); do
	sh root@node$i /sbin/service slurm restart ; 
done
----

If everything went well, you should see all the nodes show up as idle, when 
you type in sinfo:
----
# sinfo -h
viz*         up   infinite     5   idle node[1-5]
----

=== Installing/Configuring Other Software ===

You will most certainly need to install and configure various application software 
to use with VizStack. You could also install HP's Remote Graphics Software or 
TurboVNC/VirtualGL for remote visualization capabilities.

==== HP Remote Graphics Software ====

If you want to use RGS, then you will need to install the RGS package on all nodes. VizStack 
was tested with the most recent version of RGS : version 5.2.5.

NOTE: Currently, there is no mechanism to limit the set of nodes where VizStack will use RGS.

After installing RGS, you will need to do a few manual configuration steps. You need to setup 
RGS licensing as desribed in the RGS documentation.  Also, edit the file 
/etc/opt/hpremote/rgsender/rgsenderconfig, and make the following changes
----
Rgsender.Network.IsListenOnAllInterfacesEnabled=1
Rgsender.IsIloRemoteConsoleEnabled=1 (if the node is a blade workstation)
Rgsender.IsBlankScreenAndBlockInputEnabled=0
----

You also need to create a template /etc/vizstack/templates/gdm.conf file needed for RGS. 
Instructions for creating this file are in /etc/vizstack/templates/gdm.conf.template.

Note that all these changes are neeeded on every node. We recommend that you make 
the change once and propagating the changes via tools like scp OR pdcp.

The RGS sender (component that remotes the desktop) servers connections on TCP port number 
42966. You need to ensure that you configure your network stack to allow incoming connections 
on this port. This step is crucial. If you miss this, then you will be able to start remote 
desktop sessions, but users will not be able to connect to them.

==== TurboVNC/VirtualGL ====

You will need to install VirtualGL and TurboVNC on all the nodes. The following packages were 
tested with VizStack

1. turbovnc-0.5.1.i386.rpm
2. turbojpeg-1.11.x86_64.rpm
3. virtualGL-2.1.2.x86_64.rpm

The TurboVNC server listens to connections on TCP port 5901, 5902, etc. Ensure that your 
network stack allows incoming connections to these ports.  VizStack, by default configures 
each node to allow as many TurboVNC sessions as there are GPUs on the system. So, if you 
have two GPUs on the system, then you need to open TCP ports 5901, 5902. If you have 
more GPUs, then you need to open more ports.

TurboVNC needs another big setup step as well. On every node, for every user that intends to 
use TurboVNC, you need to ensure that a password file is setup.  NFS shared home directories 
may help reduce setup effort here. If a user runs the TurboVNC script without running the 
'vncpasswd' command, then they will be prompted to enter a password. The password will only 
apply to the node where the session is running at, unless you have a shared home directory 
set up.  Note that this scenario typically occurs in a demo kind of environment rather that 
in a production environment where you will already have a shared file-system in place.

==== ParaView ====

The VizStack software bundle includes a precompiled version of ParaView.  This is provided 
as a convenience for the RHEL platform. This also needs installation of MPICH2 for 
functioning.

==== Other Application Software ====

You may want to use a variety of applications on your visualization resources. You will 
need to install/configure these, install their licenses and start any daemon processes 
that may be needed.

[[REF_SSH]]
==== Configuring password-less SSH ====

Some applications (e.g. Ensight) use password-less SSH to run the parallel rendering 
parts of the application. VizStack's Amira integration script uses it to copy 
configuration files to other nodes.  Similarly, VizStack's ParaView script also uses 
password-less SSH.

Password-less SSH can be setup in a variety of other ways as well, and it is up-to 
you to use the right method for your site.

=== Configuring VizStack ===

==== Single Node Configuration ====

As root, execute the following command
----
# /opt/vizstack/sbin/vs-configure-standalone
----

This command will detect the number and type of GPUs on this node, and configure 
VizStack. It also sets up VizStack configuration needed for TurboVNC and RGS, irrespective 
of whether these are installed on the nodes.

==== Configuring on Multiple Nodes ====

In this mode, VizStack manages the visualization resources on more than one node.
VizStack is flexible enough to adapt to your environment, and meet your needs 
of configuring and managing your visualization resources.  

To use the full functionality of VizStack, you will need to configure VizStack to 
use either SLURM or password-less SSH as an application launch mechanism. 

To configure with SLURM, use

----
# /opt/vizstack/sbin/vs-configure-system -s slurm <list of nodes>
----

To configure with SSH, use
----
# /opt/vizstack/sbin/vs-configure-system -s ssh <list of nodes>
----

Note that <list_of_nodes> may be specified in the SLURM notation, or as multiple 
names on the command line. If you have GPUs on the machine where you're running 
this command from, then you need to include the hostname of this node as well.  
If your nodes are not in the default partition, then you may use the -p command 
line option and specify the partition name. 

You may want to create a setup where the nodes communicate over a network,
while a separate network carries the traffic for the remote
visualization sessions (RGS/TurboVNC). An example : your visualization nodes 
may be behind a firewall. You may not want to route the network traffic for
the remote sessions through the firewall for performance reasons. To carry this
traffic, you would need to configure another interface in all the nodes. 
To configure VizStack to use this alternate network, you may use the following 
command:
----
# /opt/vizstack/sbin/vs-configure-system -m <netmask> -s slurm <list of nodes>
----

Here, <netmask> is the TCPv4 netmask (dotted a.b.c.d) of the alternate network
interfaces (i.e. the ones want to use for remote access).

'vs-configure-system' will detect the number and type of GPUs on all the listed 
nodes, and configure VizStack to use them.  It will also set up VizStack configuration 
needed for TurboVNC and RGS, irrespective of whether these are installed on 
the nodes. The node from where this command is executed is configured to be 
the master node. You'll need to run the VizStack System State Manager (SSM) 
on this node.

[NOTE]
============================================================================
You need to ensure that the hostnames names of the nodes are resolvable 
on all the nodes where you intend to use VizStack.  This can be achieved in many 
ways: keeping /etc/hosts in sync, setting up NIS name resolution. Most cluster 
management utilities automatically do this for you.
============================================================================

[NOTE]
============================================================================
The SSM, by default, services requests on TCP port number 50000.  The other 
visualization nodes managed by VizStack need to be able to connect to the 
SSM, i.e. to this port on the master host. If your nodes are in an internal 
network without restrictions in terms of internal connections, then this is 
not a concern.  Otherwise, you may need to configure your firewall to achieve this.
============================================================================

==== Configuring SLI and QuadroPlexes ====

'vs-configure-system' cannot detect SLI connectors. If you have a display capable
QuadroPlex or SLI connectors connecting graphics cards, then you need to manually
add them to the configuration file /etc/vizstack/node_config.xml.

E.g., if you have a server connected to a QuadroPlex 2200 D2, then you would need 
to add some lines in /etc/vizstack/node_config.xml to the XML node corresponding 
to the server.
----
...
<node>
    <name>node1</name>
    ...
    <gpu>
        <index>0</index>
        ...
    </gpu>
    <gpu>
        <index>1</index>
        ...
    </gpu>
    <!-- Configure the SLI connector of the QuadroPlex. Adds an sli resource -->
    <sli>
        <index>0</index>
        <type>quadroplex</type>
        <gpu0>0</gpu0> <!-- Index of first GPU inside the QuadroPlex -->
        <gpu1>1</gpu1> <!-- Index of second GPU inside the QuadroPlex -->
    </sli>
</node>
...
----

If you have a server with two graphics cards connected to an SLI connector,
then the type of the SLI connector needs to be set to 'discrete', as follows
----
...
<node>
    <name>node2</name>
    ...
    <gpu>
        <index>0</index>
        ...
    </gpu>
    <gpu>
        <index>1</index>
        ...
    </gpu>
    <!-- Configure the SLI connector . Adds an sli resource -->
    <sli>
        <index>0</index>
        <type>discrete</type>
        <gpu0>0</gpu0> <!-- Index of first GPU connected to the SLI -->
        <gpu1>1</gpu1> <!-- Index of second GPU connected to the SLI -->
    </sli>
</node>
...
----

=== Configuring Batch Schedulers for VizStack ===

VizStack is meant for use in multi-user, multi-session environment.  Such 
environments may use schedulers to enable sharing of resources confirming to 
various site-specific policies.  Note that VizStack does not schedule jobs. 
VizStack has no direct integrations with schedulers (later releases may
 integrate directly).

You may use VizStack to manage a "remote visualization farm" configuration
 with batch schedulers. This configuration lets you assign a GPU to each 
user for use as a remote, 3D accelerated desktop.  The VizStack remote 
access scripts have a "batch" mode to aid this.

==== Example Configuration of LSF ====

Let's say you want to use LSF with VizStack. First, you need to configure 
LSF to associate GPUs as resources on the nodes which have them. GPUs, in 
LSF terms, need to be configured as resources with numeric attributes.  
GPUs as resources need to be defined in the file "lsf.shared". You'll need 
to add a line like the following:

----
gpu Numeric () Y Y (Number of GPUs)
----

This line tells LSF that "gpu" is a resource of type "Numeric" that is 
"increasing" and "consumable".

If you want to use HP RGS, then you may also add another line

----
rgs Numeric  () Y Y  (Single usage of RGS)
----

After this, you need to tell LSF which nodes have how many GPUs.  You need 
to edit your cluster definition file (lsf.cluster.<name>) for this. In the 
"resources" column for each hostname, add the number of GPUs which are 
present in the particular host.

e.g., the lines in your installation could look like
----
alpha13 ! ! 1 3.5 () () (gpu=2,rgs=1)
alpha14 ! ! 1 3.5 () () (gpu=1,rgs=1)
alpha15 ! ! 1 3.5 () () (gpu=4,rgs=1)
----

Here, alpha13, alpha14 and alpha15 have 2, 1 and 4 GPUs, respectively.  Note 
that this does not differentiate between the kind of GPUs. For now, it is 
recommended that you have the same kind of GPUs in your system.

Also, note that all nodes define "rgs=1". This means that one or zero 
instances of RGS may be active on the node. This configuration is necessary
since only one instance of RGS may be active on a node at a time.

Next, ensure that LSF picks up the changes in your configuration file.  
Also define any queues for user access, etc.

Configure VizStack using the below command. Note that you will need to 
have password-less SSH setup for root to do this.

----
# /opt/vizstack/sbin/vs-configure-system -s local -c ssh <list of nodes>
----

== VizStack Administration ==

=== Managing the SSM ===

VizStack relies on one "daemon" process to maintain the dynamic state of 
visualization resources in the system.  This daemon acts as a gatekeeper 
for access to all visualization resources.  If this program is not running, 
then no user applications (the ones using the GPUs) will be able to run. If
this program is killed, or dies, then all running visualization applications
will be terminated.

You need to start the SSM before you can run user scripts which request 
visualization resources. This is done by executing the following command 
as root from the master machine (the same machine from where you executed
the configuration command):
----
# /opt/vizstack/bin/vs-ssm start
----

To stop the SSM service, use
----
# /opt/vizstack/bin/vs-ssm stop
----

For debugging purposes, it is also possile to run the SSM in the foreground
----
# /opt/vizstack/bin/vs-ssm nodaemon
----

Typically, you would want to setup an init script to handle starting and
stopping the SSM.

=== Checking a VizStack System ===

Sometimes, you may want to find if everything is working as expected in 
the VizStack system. The command /opt/vizstack/sbin/vs-test-gpus
is provided for this purpose. By default, this program tests all the GPUs
in the system. You may restrict the program to test GPUs on a single
system by providing a list of nodes on the command line. 

Currently, this program tests if an X server can be started properly on 
each GPU. It also checks if OpenGL rendering is setup as expected.

=== Finding information ===

The vs-info command can be run by any user, and is the easiest way to
find information about the running system.

To find information about available resources, invoke it with the -r option
----
$ vs-info -r
----

To find out the available tiled displays, use
----
$ vs-info -a
----

When invoked without options, the command prints out a list of running jobs,
with their ids.

=== Terminating VizStack Jobs ===

The command vs-kill can be used to kill a job by id. vs-kill works by 
deallocating the visualization resources allocated to a job. If a scheduler
is being used, then the job is cleaned up at the scheduler level as well.

X servers started for the job are the most important part of this cleanup.
Complete job cleanup is guaranteed to happen only when you use a scheduler 
like SLURM as the application launcher. 

=== Creating and Deleting Tiled Displays ===

This is described in detail in <<X6>>.

== Using VizStack ==

VizStack provides a few scripts to aid usage of the visualization resources. 
Currently, there are two categories of scripts: remote visualization 
scripts(RGS, TurboVNC/VirtualGL) and application launch scripts(Amira/ParaView)

Many application scripts need access to a Tiled Display to run. Tiled Displays 
need to be configured prior to running these.

=== Remote Desktop Sessions ===

VizStack provides a GUI for starting remote desktop sessions.
The GUI is installed on the end user's desktop. This allows users
to easily start remote desktop sessions(using RGS/TurboVNC). 

VizStack also provides a command line interface using scripts, one
each for TurboVNC and RGS. To use these, users have to log into the 
VizStack system and then invoke the script. Several options are 
currently available only from the command line interface.

==== Installing the GUI interface ====

VizStack provides the GUI interface for both Windows and Linux.

For Windows, an all-in-one executable setup program is provided.
This can be run by the end users to install the interface for
starting remote desktop sessions using HP's RGS software as well
as TurboVNC/VirtualGL.

An RPM is provided for Linux systems. The RPM depends on the 
following software, which needs to be installed prior to installing
the RPM:

1. python-paramiko
2. wxPython

NOTE: The end user needs to install HP RGS Receiver or TurboVNC client software prior to using these remote access tools.

==== HP Remote Graphics Software ====

===== Using the GUI =====

On Windows, click the Start button, select "All Programs", then
select "VizStack Remote Access", and finally click on 
"Viz Connector for HP RGS Receiver". This will show up a GUI as
shown in <<X3>>.

On Linux, the user needs to run the command
----
$ /opt/vizrt/bin/remotevizconnector
----

[[X3]]
.Starting a Remote Desktop session using HP RGS
image::../images/rgs-connector.png[Session using RGS]

At this time, the user needs to type in their username and the
hostname (or IP address) of the VizStack master, and click on the
'Go' button. After clicking the 'Go' button, the user will be
prompted for their account password. This authentication can be 
avoided by configuring the PuTTY Agent to allow passwordless
SSH into the VizStack master node. Finally, the RGS receiver 
will be started and the user will be authenticated again.

If all goes well, the desktop will start with a default resolution
of 1280x1024 pixels. This can be changed by clicking on the 
'More Options' checkbox, and selecting a desired resolution.

To give up the session, the user needs to logout of the desktop.
The user may also choose to click the 'Terminate Session' button.
Doing so will kill all running applications and is not recommended.

===== Using viz-rgs =====

The script "viz-rgs" allocates remote desktop sessions using HP RGS 
software. You need to run this script as a regular user; it will not 
run as root. Sample invocations are shown below.

Allocate an RGS desktop on a single GPU, picking up a node automatically. 
Note that the node where the RGS desktop is running can use the other 
GPUs (i.e. excluding the one which is in use by RGS)

----
$ viz-rgs
----

This script will print out the connection information that the user needs 
to enter into the RGS client program. After authentication, the user 
will be able to use the desktop. User gives up the desktop either by logging 
out or by killing the script (former method is recommended).

Start an RGS desktop on a GPU with a specific desktop size

----
$ viz-rgs -g 1600x1200
----

Note that this resolution does not need to match any physical display device 
resolution.  The only restriction is that the width of the desktop must 
be a multiple of 8.  E.g, the following is allowed too

----
$ viz-rgs -g 1000x1000
----

Desktop sizes can go really large: up-to 8192x8192 on Quadro FX 5800/4800/1800,
and 4096x4096 on 5600/4600/1700. Note that large desktops can directly result 
in sluggish performance and higher bandwidth usage.

Desktop sessions started using viz-rgs are completely secure. This is 
achieved by running the applications in a virtual frame buffer. The desktop pixels 
on these sessions is never sent to the display output on the graphics card. 

===== Usage with Batch Schedulers =====

With LSF configures as described earlier, use the following command to start
a session
----
$ bsub -R "rusage[gpu=1,rgs=1]" /opt/vizstack/bin/viz-rgs -b
----

viz-rgs will allocate a GPU on the node where the script gets scheduled, 
and setup RGS to use that GPU.  However, it will not do any site specific 
operations that may be needed to inform the user how this session may be 
used (e.g. by sending email). You may choose to redirect the output of 
the script to a file, and get the connection information from there.

[NOTE]
============================================================================
The script will allocate an RGS session on the node irrespective of whether
you reserve a GPU. This happens since VizStack cannot query LSF and find out
what resources were allocated to a particular user.  

Later releases of VizStack may be able to query the scheduler about what 
resources were allocated to a node, and then use that information to require 
the user to reserve GPUs using LSF/other schedulers.
============================================================================

==== TurboVNC/VirtualGL ====

===== Using the GUI =====

On Windows, click the Start button, select "All Programs", then
select "VizStack Remote Access", and finally click on 
"Viz Connector for TurboVNC". This will show up a GUI, as shown
in <<X4>>.

On Linux, the user needs to run the command
----
$ /opt/vizrt/bin/remotevizconnector --tvnc
----

[[X4]]
.Starting a Remote Desktop session using TurboVNC/VirtualGL
image::../images/tvnc-connector.png[Session using RGS]

At this time, the user needs to type in their username and the
hostname (or IP address) of the VizStack master, and click on the
'Go' button. After clicking the 'Go' button, the user will be
prompted for their account password. This authentication can be 
avoided by configuring the PuTTY Agent to allow passwordless
SSH into the VizStack master node. Finally, the TurboVNC viewer 
will be started and the user will be authenticated again.

If all goes well, the desktop will start with a default resolution
of 1280x1024 pixels. This can be changed by clicking on the 
'More Options' checkbox, and selecting a desired resolution.

To give up the session, the user needs to logout of the desktop.
The user may also choose to click the 'Terminate Session' button.
Doing so will kill all running applications and is not recommended.

===== Using viz-tvnc =====

The script viz-tvnc allocates remote desktop sessions using TurboVNC.  With the 
default VizStack configuration, a single node can host as many TurboVNC sessions 
as there are GPUs in the machine.

Usage is same as RGS,
----
$ viz-tvnc
----

This will allocate a TurboVNC session. You need to use the TurboVNC client program 
to connect to the desktop.  To run OpenGL applications inside this session, use 
the following command
----
$ vglrun <appname> <args>
----

Again, viz_tvnc can be run with any arbitrary resolution, subject to the "multiple 
of 8" restriction.

===== Usage with Batch Schedulers =====

If you have configured LSF as mentioned earlier in this document, a user may start 
a TurboVNC session using
----
$ bsub -R "rusage[gpu=1]" /opt/vizstack/bin/viz-tvnc -b
----

Note that viz-tvnc will allocate a GPU on the node where the script gets scheduled, 
and setup TurboVNC to use that GPU.  However, it will not do any site specific 
operations that may be needed to inform the user how this session may be used. 
You may choose to redirect the output of the script to a file, and get the 
connection information from there. 

A better method would be to use TurboVNC's reverse connection feature. In this
method, the user first starts the TurboVNC client in "listening" mode on his 
desktop and then run the following command on the cluster:
----
$ bsub -R "rusage[gpu=1]" /opt/vizstack/bin/viz-tvnc -b -c <desktop_host>
----

=== Popular Applications ===

==== Avizo ====

VizStack provides a script that makes it easy to run MCS Avizo on a tiled display. 
VizStack's Avizo script relies on prior setup of passwordless SSH. To run Avizo 
on a tiled display, run

----
# viz-avizovr -t tile2x2
----

This works, provided there is a tiled display named "tile2x2"

==== ParaView ====

The VizStack ParaView script needs mpich2 to be installed and configured. The relevant 
rpms for RHEL5.x are

mpich2-devel-1.0.8p1-2.el5.x86_64.rpm
mpich2-libs-1.0.8p1-2.el5.x86_64.rpm
mpich2-1.0.8p1-2.el5.x86_64.rpm

To run ParaView on a tiled display, run

----
# viz-paraview -t tile2x2
----

This works, provided there is a tiled display named tile2x2. ParaView has been only 
tested on Redhat as there are some core issues compiling ParaView on SLES 10.2.

The ParaView script has 2 modes of operation, Distributed Rendering (DR) mode and 
Tiled Display(TD) mode. 

In DR mode, render GPUs are specified using the -r option and the display mode of 
the X server with the -m option. Now the loaded dataset is rendered in a distributed 
mode by ParaView.

In the TD mode, a Tiled Display specified in /etc/vizstack/resource_group_config.xml,
is specified on the command-line using -t option. The loaded dataset is displayed
on this tiled display.

ParaView scripts has a server only mode, where only the servers are started on
the allocated tiled display. The node name and port number where one should connect
to the server is printed on the command-line. The user can start the ParaView client
on his local desktop (it can be running either Windows or Linux) and connect to
the ParaView server. Note that the firewall on the node on which the server is running
needs to let in incoming connections on the specified port.

ParaView works by using ports within the range 11111 - 11143. The firewall needs to
be configured to open these ports when ParaView script is run in server only mode.


=== Local Desktop ===

Sometimes, the user would want to directly interact with one OR more GPUs using an 
X server with a directly connected keyboard/mouse. This is equivalent to using the 
GPUs and Input devices as if they were part of a local desktop.  

If you want to use such a local desktop setup, then you need to use the viz_desktop 
script. This script accepts a Tiled Display as an input argument. The tiled display 
needs to be setup such that a keyboard and a mouse are part of the tiled display 
specification.  The tiled display can use as many GPUs as needed from a single 
machine.  The input tiled display is restricted to include GPUs only from a single 
machine.  If you pass a Tiled Display which does not adhere to these restrictions, 
then the script will not start the desktop.

Starting a local desktop is as easy as running the other scripts
----
# viz-desktop -t desktop
----

== VizStack Numbering Conventions ==

VizStack has a convention for numbering the visualization resources on each node 
in a particular order.  Knowing this numbering convention is important to do 
certain tasks like:

1. Configuring Tiled Displays to use specific GPUs

2. Configuring the firewall for remote visualization services like TurboVNC and
HP RGS

=== VizStack GPU Numbering ===

Systems controlled by VizStack may have one/more GPUs on each node.  It is 
not necessary that all the nodes have the same configuration.  In particular, 
this means that a

1. a node is not restricted to have the same number of GPUs as another node

2. a node does not need to have the same kind of GPUs as another node.

3. on a node, you may choose to connect GPUs on any available slots, independent 
of how you may have connected the GPUs on any other node

4. GPUs can be connected to display devices in any layout you choose.  Again, 
the display connections on a node need not match the connections on another node.

VizStack provides you the flexibility to configure nodes independently depending 
on how you intend to use the visualization resources.  However, it also introduces 
the problem of addressing.  Suppose you need to drive a tiled display or a CAVE. 
 You may physically connect the outputs of GPUs on one/more nodes to the display 
devices.  However, you still need to tell VizStack what GPUs have been connected 
to displays.

VizStack identifies each GPU on a node via a number, called the "index".  An 
index that applies to a GPU is a "GPU index".  The index of a GPU on a node can 
range from 0 to (number of GPUs on the system)-1.  Note that all GPUs on the system 
are assigned a unique index.  The GPUs could be discrete GPUs connected to PCI slots, 
or may be included inside external QuadroPlex graphics devices.

Each GPU is assigned an index when VizStack's configuration script is run. The index 
of a GPU depends on three things

1. The PCI slot the GPU is sitting on

2. If the GPU is inside a QuadroPlex, then the GPUs position in the QuadroPlex as 
well as the PCI slot where the QuadroPlex is connected do matter as well.

3. The other GPUs (including those inside QuadroPlexes) that are connected to the system

VizStack uses the following algorithm to number the GPUs

1. Find all the GPUs on the system

2. Order the GPUs by their PCI id.

3. The index of a GPU is its position in the sorted list.

While this is fine from a system point of you, you would want to associate the 
GPU index with a physical GPU.  Determining the physical position of a GPU by 
index is not difficult.  

First, we will consider the case where you have only Quadro FX GPUs installed 
on the machine.  With the machine in the rack mounted configuration, just count 
the GPUs from left to right.  GPU index 0 is the leftmost GPU, GPU index 1 is 
the next GPU to its right, GPU index 2 is the next GPU, and so on.

Next, consider the case of QuadroPlex D series external graphics boxes.  These 
connect to the machine via a HIC (Host Interface Card) that is plugged into a 
PCI express slot.  For performance reasons, we recommended that you use a PCIe 16x 
slot where possible. A single machine may have one or more QuadroPlexes connected 
to it, subject to slot limitations.  First locate the HIC corresponding to the 
QuadroPlex on the system. Next count from left to right again. The index of a GPU 
is the number of GPUs preceeding it when counting from the left.  A HIC is counted 
equal to the number of GPUs inside the QuadroPlex connected to it.  Inside a 
QuadroPlex, the GPUs are again counted from left to right (when looking from the rear)

=== GPU Display Output Numbering ===

VizStack works with a variety of nVidia GPUs.   VizStack can use the GPUs to drive a 
variety of display devices.  VizStack can also drive one/more of a GPUs display 
outputs at the same time.  VizStack uses a convention to refer to the display outputs. 

[[X5]]
.Display Numbering on GPUs
image::../images/gpu-display-port-mapping.png[Display Numbering on GPUs]

<<X5>> shows how VizStack numbers the port numbers on a variety of GPUs.  Note that
the convention varies across the generation of GPUs. The convention used for the 
QuadroFX 5600 applies to the GPUs inside the QuadroPlex 2100 D2 as well.  Note that 
the picture shows the GPUs as you would see them from the back of a server/workstation.

[[X6]]
== Configuring Tiled Displays ==

A tiled display can be driven using the display outputs from a single GPU, multiple 
displays from multiple GPUs on the same node, as well as GPUs spread across multiple
 nodes.  Optionally, these displays could be running in stereo mode according to 
the capabilities of the display device.  Additionally, Xinerama 
may be used over one or more GPUs. You would typically use Xinerama either for 
application compatibility, or to get a single large desktop. Finally, you may 
choose a rotation mode for all the display devices included in the tiled display.

Tiled displays are identified by unique names in VizStack. Typically, an administrator
would assign meaningful names to tiled displays, such that users are not confused
about which tiled display to use.

VizStack tiled displays are made up of elements called "blocks". Two types of
blocks are currently supported : GPU and QuadroPlex. A GPU block utilizes
a single GPU to drive one or two displays. A QuadroPlex block uses QuadroPlex
D series external graphics to drive two to four displays, using the SLI mosaic
 mode. A tiled display may consist of blocks of only one type, all of the
blocks are either GPUs or QuadroPlexes. Note that the GPUs inside a QuadroPlex
can be used as independent GPU blocks.

Blocks can drive one or more displays according to the type of block.  GPU blocks
can drive one or two displays in two possible layouts

1. A single display (1x1)
2. Two displays side-by-side (2x1), or one below the other (1x2).

.Possible Display Layouts for a GPU block
image::../images/gpu-block.png[Possible Display Layouts for a GPU block]

QuadroPlex blocks can drive two to four displays in the following possible layouts

1. Two displays arranged horizontally (2x1) or vertically (1x2)
2. Four displays in a square configuration (2x2)
3. Three displays arranged horizontally (3x1) or vertically (1x3)
4. Four displays arranged horizontally (4x1) or vertically (1x4)

.Possible Display Layouts for a QuadroPlex block
image::../images/qp-block.png[Possible Display Layouts for a QuadroPlex block]

Tiled Displays restrict usage of a single type of display device. It is assumed that
the same type of display device is connected to all GPUs. Further, all display devices
are setup with the same display mode. VizStack comes defined with a few display 
devices. Each display device defines a set of modes that are compatible with it.

All display may also be physically rotated, in one of the following ways:

1. Portrait (90 degrees to the left)
2. Inverted Portrait (90 degrees to the right)
3. Inverted Landscape (180 degrees)

Note that the same rotation setting applies to all the displays.

All displays may also be configured for one of the following stereo modes:

1. Active stereo, using Shutter glasses
2. Passive stereo. This mode is compatible only with GPU blocks, and is not
available for use with QuadroPlex blocks.
3. Auto-stereoscopic SeeReal DFP, suitable for use with Tridelity SV Displays.
4. Auto-stereoscopic Sharp3D DFP

Tiled displays are made by replicating blocks in a rectangular matrix. In general,
a tiled display may consist of 'n'\*'m' blocks, where 'n' is the number of columns,
and 'm' is the number of rows. A tiled display thus uses n\*m blocks.

Typically, VizStack configures one X server per block. However, with GPU blocks, 
it is possible to use two or more GPU blocks per X server. X servers can be 
configured with GPUs in a rectangular layout. The number of GPUs per X server 
is the same across all X servers. The X server can be setup with Xinerama enabled,
thus exposing only one X screen covering all GPUs.

This is useful in at-least the following two scenarios:

1. Some applications may not be able to use more than one GPU per node (or more
specifically, an X screen) . This limitation can partly be overcome by using 
QuadroPlex blocks, thus enabling the application to run on two GPUs. However,
this is not a solution if you need to run your application over more than two
GPUs. Also, you may not have a QuadroPlex. One way to get around
these limitations is to enable Xinerama on the X server, thus enabling the 
application to run on all the GPUs. However, this does have drawback : performance
with Xinerama degrades as the number of GPUs increases.

2. You may want to configure the X server with input devices, and physically
control all the GPUs configured for the X server.

=== Creating Tiled Displays ===

VizStack provides a command line tool to configure tiled displays. This tool is
found in the directory for administrative tools, /opt/vizstack/sbin, and is called
'vs-manage-tiled-displays'. This tool is menu driven, and allows the administrator
to create and delete tiled displays. Options are provided to list the
available tiled displays, as well as to show the configuration of available tiled 
displays.

=== Tiled Display Examples ===

VizStack provides a lot of flexibility w.r.t configuring Tiled Displays,
and that increases the complexity involved. These possibilities can be
a source of confusion. So we show a few Tiled Display configurations
to illustrate some possibilities.

The easiest way to understand tiled displays is to realize that they are
made by replicating blocks (with a given display layout) in a regular 
grid. This is illustrated in <<X1>> and <<X2>>. If you choose to apply
any rotation, then all the blocks will be rotated in a similar fashion.
If you choose any stereo mode, then all tiles will run in the same
stereo mode.  For passive stereo, there is an additional restriction:
you can use passive stereo only with GPU blocks driving a single display.
If you use passive stereo, then two outputs from the GPU will be used
to drive the left and right stereo images.

[[X1]]
.Tiled displays using GPU blocks
image::../images/gpu-td-examples.png[Tiled displays using GPU blocks]

[[X2]]
.Tiled displays using QuadroPlex blocks
image::../images/qp-td-examples.png[Tiled displays using QuadroPlex blocks]

=== Frame Lock Considerations ===

VizStack supports the Frame Lock feature that can be used with nVidia high
end graphics cards and QuadroPlex external graphics solutions. To use Frame Lock
with graphics cards, you will need to connect the optional G-Sync card to the
graphics cards. QuadroPlexes are configured with internal Frame Lock devcices.

If you want to frame synchronize multiple graphics cards, then the G-Sync cards
connected to those graphics cards need to be chained together, typically done
using ethernet cables (or more accurately, CAT-5 or CAT-6 cables). Care must be
exercised not to connect the cables to ethernet switches or cards. Doing
so can damage the G-Sync cards.  A G-Sync chain typically has one master and
multiple slaves. 

G-Sync chains are fairly static configurations. Typically you decide the
configuration of the tiled displays in advance, and chain the G-Sync cards.

We recommend that administrators setup tiled displays with the following
 considerations in mind:

1. Avoid including GPUs from more than one frame lock chain in a tiled
display. Doing so may result in an undefined state w.r.t framelock.

2. Avoid creating multiple tiled displays on a single framelock chain,
unless you intend to use them together.

User scripts provided by VizStack automatically enable frame lock on tiled 
displays, if frame lock is available on all of them. Failure to setup frame 
lock is treated as a failure, and the scripts terminate. If you create
tiled displays which cannot be framelocked, and the scripts terminate
with framelock errors, then you should use the command line option 
'--no-framelock' so that the scripts avoid trying to setup framelock.

=== Tiled Displays with Input Devices ===

You may want to setup tiled displays so that users can interact with them
using the connected keyboard and mouse on the machine.

If you want only one physical user per node, then things should work fine
for you. Note that a user accessing a node using HP's Remote Graphics 
Software should be counted as a physical user for this purpose.

VizStack allows multiple physical users per node (not just one). By
default, VizStack's configuration script configures one system-wide keyboard
and one system-wide mouse for every node. When multiple users are trying
to share a node, you need to physically install separate input devices 
(keyboard and mouse) for them. You will also need to define these as
resources in /etc/vizstack/node_config.xml on the master node. 
You will also need to configure your tiled display to use these input
devices.

==== Configuring a Keyboard ====

Connect the keyboard to the node where you want to use it. This section
assumes configuration of a USB keyboard.

SSH into the node, and look in the file /proc/bus/input/devices. You must
see lines like the following corresponding to your connected keyboard
----
I: Bus=0003 Vendor=03f0 Product=0024 Version=0300
N: Name="CHICONY HP Basic USB Keyboard"
P: Phys=usb-0000:00:07.1-1.4.1/input0
S: Sysfs=/class/input/input2
H: Handlers=kbd event2
B: EV=120003
B: KEY=1000000000007 ff87207ac14057ff febeffdfffefffff fffffffffffffffe
B: LED=7
----

Make a note of the "Phys" value. You may now define this keyboard in
/etc/vizstack/node_config.xml as follows:
----
...
<node>
    <name>node1</name>
    ...
    <gpu>
        ....
    </gpu>
    <!-- Existing System Keyboard. -->
    <keyboard>
        <index>0</index>
        <type>SystemKeyboard</type>
    </keyboard>
    <!-- New keyboard being defined by us -->
    <keyboard>
        <index>1</index>
        <type>USBKeyboard</type>
        <phys_addr>usb-0000:00:07.1-1.4.1/input0</phys_addr>
    </keyboard>
    ...
</node>
...
----

You'll need to restart the SSM to use the new keyboard.

==== Configuring a Mouse ====

Connect the mouse to the node where you want to use it. This section
assumes configuration of a USB mouse.

SSH into the node, and look in the file /proc/bus/input/devices. You must
see lines like the following corresponding to your connected mouse
----
I: Bus=0003 Vendor=046d Product=c016 Version=0340
N: Name="Logitech Optical USB Mouse"
P: Phys=usb-0000:00:07.1-1.4.2/input0
S: Sysfs=/class/input/input3
H: Handlers=mouse1 event3
B: EV=7
B: KEY=70000 0 0 0 0
B: REL=103
----

Make a note of the "Phys" value. You may now define this mouse in
/etc/vizstack/node_config.xml as follows:
----
...
<node>
    <name>node1</name>
    ...
    <gpu>
        ....
    </gpu>
    ....
    <!-- Existing System Mouse. -->
    <mouse>
        <index>0</index>
        <type>SystemMouse</type>
    </mouse>
    <!-- New mouse being defined by us -->
    <mouse>
        <index>1</index>
        <type>USBMouse</type>
        <phys_addr>usb-0000:00:07.1-1.4.2/input0</phys_addr>
    </mouse>
    ...
</node>
...
----

You'll need to restart the SSM to use the new mouse.

== Using Display Devices with VizStack ==

The following display devices have built-in definitions defined in VizStack

1. HP LP3065
2. HP LP2065
3. Tridelity MV 2700. This is Tridelity's 27" multi-view autostereoscopic display.
This is internally based on a Samsung SyncMaster display.
4. HP AVO Smart Cable; this is HP's standard KVM dongle
5. HP L1955
6. Sony SRX Projector

Using other display devices will need you to configure them specifically for VizStack.

=== Configuring site-specific Display Devices ===

VizStack can be configured to use a variety of display devices: Stereo Projectors, 
TFT monitors, CRT Monitors, and in general any display device. VizStack comes 
with built-in definitions for a few display devices. If VizStack already defines 
a template for your display device, then you are in luck. Otherwise, don't 
worry - it is not hard to setup VizStack to use your display device.

Before VizStack can use a specific kind of display device, you need to create 
what is called a display device template for it. Creating the display device template 
lets you define settings for a display device once and reuse the settings later 
for any number of display devices of the same kind.

Most display devices can describe their capabilities to a graphics card, using a 
data structure called Extended Display Identification Data (EDID). The graphics 
card uses this information to determine what modes are supported on the display 
device, as well as to compute the display signal timing for the supported modes.

If a graphics card is directly connected to a display device (using either a 
VGA or DVI cable), then the graphics card will get access to the EDID. If the 
connection is made a DVI extender, then the graphics card may not have access 
to the EDID. DVI extenders come in two types: ones that support DDC (Display 
Data Channel) protocol and once that don't. If you use a DVI extender which supports 
DDC, then you may treat the connection as a local connection. If your DVI extender 
does not support DDC, then you will need to get the EDID of the display device manually. 

One way to get the EDID is to use a laptop with Windows installed on it. Connect the 
display device to the laptop, and configure Windows to enable/drive it. Make sure 
that windows can detect the details of the display device (name of the device, etc). 
The next step is extracting the actual EDID information about the device. One way to 
get the actual EDID data is by downloading the softMCCS tool (available from 
http://www.entechtaiwan.com/lib/softmccs.shtm). To extract the EDID file for the 
connected display, 

1. Run softMCCS.
   image:../images/softMCCS.png[Screenshot of softMCCS]
2. Select the display device that you connected on the drop down list
3. Select File | Save As and save the file at a convenient location
4. Copy the file over to the master visualization node. If you use FTP, then ensure that you copy the file as a binary file.

After you get an EDID file, then you need to create an XML template file for the 
display. Examples are it /etc/vizstack/templates/displays. Note that the display 
templates need to be propagated to each node, and need to be available at the 
same path on all the nodes.

== The VizStack Python API ==

A major part of VizStack is the API. This API is written in the popular programming 
language, Python. This API is used heavily in VizStack. All application scripts, as 
well as the remote access scripts are written using it.

The VizJob API exposes an object oriented interface to work with visualization 
resources. VizStack treats a system as a collection of the following types of 
visualization resources,

1. GPUs (Graphics Processing Units) : QuadroFX graphics cards, as well as GPUs inside QuadroPlexes
2. SLI bridges: discrete SLI bridges and ones internal to a  QuadroPlex
3. Input Devices : keyboards, mice
4. X Servers : normal and virtual X servers

These resources are exposed directly to programmers using the vsapi. Using the vsapi, 
a programmer can

1. Allocate a set of resources constituting a visualization job
2. Free the resources when they are no longer needed
3. Determine what resources are allocated to which jobs.
4. Configure X servers in a way that matches application needs
   a. with one or more screens
   b. each screen can control one or more GPUs
   c. GPUs can be configured to drive display devices configured with VizStack
   d. SLI modes, stereo can be setup
   e. Optionally, a keyboard and mouse can be used as input devices
   f. all resources can be configured independently of each other
5. Start and stop X servers when needed
   a. more than one X server can be run used on a node
6. Run application components on chosen X servers/screens
7. Enable/disable frame lock
8. Query VizStack's system configuration. This allows an application to enumerate 
all available resources, and determine what resources may be suitable for the application.
9. Query runtime configuration

The VizStack scripts viz-rgs, viz-tvnc, viz-avizovr and viz-paraview use the VizStack 
API, and serve as examples of usage. Documentation for the API can be obtained using 
regular python mechanisms. 

Example of getting documentation on the python command line
----
>>> import vsapi
>>> help(vsapi.ResourceAccess)
>>> help(vsapi.Server)
>>> help(vsapi.GPU)
>>> help(vsapi.Screen)
----

The directory /opt/vizstack/share/samples contains many commented examples that use 
the vsapi in a variety of interesting ways.

== VizStack Configuration Files ==

VizStack uses XML for all configuration files. 

=== Tiled Displays ===

Tiled Displays are defined in the file /etc/vizstack/resource_group_config.xml.

Tiled Displays are defined as a "resource group" specification. A resource
group is a generalized mechanism in VizStack to select resources. Tiled Displays
are Resource Groups with a handler 'tiled_display' which indicates that the resource
group is a tiled display.

=== Tiled Display Parameters ===

The configuration of each Tiled display is decided by a few parameters.

==== block_type ====

The block_type parameter represents the type of block that is used to 
build the tiled display. Two values are currently recognized, "gpu" and
"quadroplex".

==== num_blocks ====

num_blocks is a two element list [n,m], where n is the number of columns,
 and m is the number of rows.

==== block_display_layout ====

This parameter defines how each block drives displays. gpu blocks can drive one
or two displays with the following possible layout values

1. [1,1] : Single display
2. [2,1] or [1,2] : Two displays arranged horizontally or vertically

quadroplex blocks can drive two to four displays with the following possible 
layout values

1. [2,1] or [1,2] : Two display arranged horizontally or vertically
2. [2,2] : Four displays in a square configuration
3. [3,1] or [1,3] : Three displays arranged horizontally or vertically
4. [4,1] or [1,4] : Four displays arranged horizontally or vertically

==== display_device ==== 

This parameter defines the type of display device connected to each tile. Note that
the same type of display device is assumed to be connected to all tiles.

==== display_mode ====

The display mode to be used on the display device. If this is not specified, then
the default display mode for the device is used.

==== stereo_mode ====

This parameter defines what stereo mode is used on the tiles. The possible values
are

1. active : Active stereo, with stereo glasses
2. passive : Passive stereo. This mode cannot be enabled with a quadroplex block
3. SeeReal_Stereo_DFP : For use with autostereoscopic DFPs.
4. Sharp3D_Stereo_DFP : For use with autostereoscopic DFPs.

==== combine_displays ====

This boolean value enables/disables usage of Xinerama on the X servers. 

==== group_blocks ====

This parameter specifies the arrangement of the GPUs inside the individual X servers
: number of columns by number of rows. Use this parameter to group GPUs together,
and specify their layout inside an X server.

==== remap_display_outputs ====

Causes VizStack to change the order in which it drives the display output ports.

==== rotate ====

Use this parameter to define the physical orientation of the display devices. Possible
values are

1. none
2. portrait
3. inverted_portrait
4. inverted_landscape

==== A Single Tile ====

This is simplest example of a single tiled display - i.e., a single display. 
It is presented here for introducing the basic concepts.

image:../images/td-single-schematic.png[Single Tile Display]

The XML needed to define this is 

----
<resourceGroup>
    <name>simple<name>
    <handler>tiled_display</handler>
    <handler_params>
        block_type="gpu";
        num_blocks=[1,1];           # We use 1*1 = 1 GPU
        block_display_layout=[1,1]; # Each GPU drives a single display
        display_device="LP3065";    # Each display connected to an LP3065 monitor
    </handler_params>
    <resources>
        <reslist>
            <res><serverconfig><hostname>node1</hostname><serverconfig></res>
            <res><gpu><hostname>node1</hostname><index>0</index></gpu></res>
        </reslist>
    </resources>
</resourceGroup>
----

This XML defines a tiled display called "simple" that drives a single 
LP3065 monitor using GPU-0 installed on the node named node1.  Note that 
no specific mode is specified for the display device, so it will run at its 
default resolution and refresh rate i.e. 2560x1600@60 Hz. Also, note the display 
is connected to Port 0 of the GPU (see picture).

The tiled display is called "simple", you would use it with Avizo by using the command line

	# viz_avizovr -t simple

The XML for this resource group defines the handler as tiled_display. Currently 
this is the only supported value. This means that the current resource group defines 
a handler of type "tiled display".

The string inside handler_params defines the arguments for the tiled display. You 
have a choice of specifying one/more parameters here.  The string is processed as 
a python code fragment, so you may specify various kinds of values (strings, lists,
etc) inside the string.  For reasons of security, function calls are not allowed 
inside the handler parameters.

You may specify other parameters for the tiled display as python variables inside 
handler_params.

==== 2x1 Display Layout from one GPU ====

Let's say you want to drive two monitors side by side from a single GPU, 

image:../images/td-2x1-schematic.png[Single Tile Display]

The XML needed for this would be. 
----
<resourceGroup>
    <name>simple<name>
    <handler>tiled_display</handler>
    <handler_params>
        block_type="gpu";
        num_blocks=[1,1];           # We use 1*1 = 1 GPU
        block_display_layout=[2,1]; # Each GPU drives two displays side by side
        display_device="LP3065";    # Each display connected to an LP3065 monitor
    </handler_params>
    <resources>
        <reslist>
            <res><serverconfig><hostname>node1</hostname><serverconfig></res>
            <res><gpu><hostname>node1</hostname><index>0</index></gpu></res>
        </reslist>
    </resources>
</resourceGroup>
----

Note that the only change from the previous example is that
block_display_layout is changed to [2,1].

VizStack configures the two display outputs from the GPU to drive a single large 
framebuffer. This is implemented by configuring the X server is to drive a single 
"X screen" with the effective  resolution of the two displays.  Note that changing 
"block_display_layout" from [2,1] to [1,2] would result in the displays being setup 
one below the other.

==== 2x2 Layout from two GPUs on one node ====

Each GPU can be configured to drive a 2x1 layout to achieve this, as below:

image:../images/td-2x2-schematic.png[2x2 Layout]

The XML needed for this would be
----
<resourceGroup>
    <name>simple<name>
    <handler>tiled_display</handler>
    <handler_params>
        block_type="gpu";
        num_blocks=[1,2];           # We use 1*2 = 2 GPUs
        block_display_layout=[2,1]; # Each GPU drives two displays side by side
        display_device="LP3065";    # Each display connected to an LP3065 monitor
    </handler_params>
    <resources>
        <reslist>
            <res><serverconfig><hostname>node1</hostname><serverconfig></res>
            <res><gpu><hostname>node1</hostname><index>0</index></gpu></res>
            <res><gpu><hostname>node1</hostname><index>1</index></gpu></res>
        </reslist>
    </resources>
</resourceGroup>
----

Note that num_blocks gets a new value equal to [1,2].  Note that we also ask 
for an extra GPU, GPU #1 on "node1".

==== 2x2 Layout from two GPUs from two nodes ====

image:../images/td-2x2-2node-schematic.png[2x2 Layout from Two Nodes]

In this configuration, two nodes node1 and node2 are driving the 4 displays, two each 
from one GPU on each node.  Note that the GPUs on the nodes are not at the same 
location: GPU0 comes from node1 and GPU1 comes from node2. The XML needed for this would
be

----
<resourceGroup>
    <name>simple<name>
    <handler>tiled_display</handler>
    <handler_params>
        block_type="gpu";
        num_blocks=[1,2];           # We use 1*2 = 2 GPUs
        block_display_layout=[2,1]; # Each GPU drives two displays side by side
        display_device="LP3065";    # Each display connected to an LP3065 monitor
    </handler_params>
    <resources>
        <reslist>
            <res><serverconfig><hostname>node1</hostname><serverconfig></res>
            <res><gpu><hostname>node1</hostname><index>0</index></gpu></res>
        </reslist>
        <reslist>
            <res><serverconfig><hostname>node2</hostname><serverconfig></res>
            <res><gpu><hostname>node2</hostname><index>1</index></gpu></res> # Note: we're using GPU #1
        </reslist>
    </resources>
</resourceGroup>
----

==== 2x1 layout from two GPUs on two nodes ====

image:../images/td-2x1-2nodes-schematic.png[2x1 Layout from Two Nodes]

You may also drive two display devices from two nodes, in a side by side 
layout.  The XML needed is shown below. 

----
<resourceGroup>
    <name>simple<name>
    <handler>tiled_display</handler>
    <handler_params>
        block_type="gpu";
        num_blocks=[2,1];           # We use 2*1 = 2 GPUs
        block_display_layout=[1,1]; # Each GPU drives one displays
        display_device="LP3065";    # Each display connected to an LP3065 monitor
    </handler_params>
    <resources>
        <reslist>
            <res><serverconfig><hostname>node1</hostname><serverconfig></res>
            <res><gpu><hostname>node1</hostname><index>0</index></gpu></res>
        </reslist>
        <reslist>
            <res><serverconfig><hostname>node2</hostname><serverconfig></res>
            <res><gpu><hostname>node2</hostname><index>1</index></gpu></res>
        </reslist>
    </resources>
</resourceGroup>
----

Note that block_display_layout is now set to [1,1] to indicate that 1 
display will be driven by each GPU. num_blocks is changed to [2,1] to 
indicate the horizontal layout.

Changing num_blocks to [1,2] would convert this into a 1x2 vertical layout.

==== Altering the order in which VizStack drives displays ====

GPU managed by VizStack may have 2 (FX5600, etc) or 3 display outputs(FX5800, etc). 
In all the examples till now, the displays have been driven from the graphics card 
output ports in a certain order. E.g., if you set display_block_layout to [2,1], 
then you effectively ask for Display Output 0 to be connected to a display device 
and Output 1 to be connected to a display device that is positioned to the right 
of the other display.  

Sometimes, you may want to change this order. For instance, if you have a console 
attached to Display Output 0, you would want to avoid VizStack from displaying the 
output on port 0. Another reason could that you want to use a rectangular tiled 
display in various ways without modifying the wiring.

VizStack's tiled displays support a parameter "remap_display_outputs", which lets 
you do this. "remap_display_outpus" is a list of port numbers. VizStack sends the 
output normally intended port port #n to the port specified by the nth element in 
the list.

Some illustrative examples follow

[format="dsv",separator="|",frame="all",grid="all"]
.20`80~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Value |Effect
[0,1] | No effect
[1]   | Display that would have normally been driven via port 0 \
is driven via port 1. 
[1,0] | Output normally sent to port 0 is sent to port 1, and \
output that would have been sent to port 1 is sent to port 0. \
Essentially, this swaps the positions of the two displays. 
[1,2] | Send the display output normally sent to port 0 to \
port 1, and output normally sent to port 1 to port 2. This \
effectively lets you drive a passive stereo pair from a single \
FX 5800, while still leaving the console free to be used otherwise.\
Note that port #2 is only available on the FX 5800, 4800, 3800 \
and 1800 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Note that remap_display_outputs can only have as elements as needed for the 
specified block_display_layout. Alternatively, remap_port_index must have exactly 
block_display_layout[0]*block_display_layout[1] elements. So, if you have 
display_block_layout=[2,1], you'll need to have exactly two elements in remap_port_index.

===== Using Display Output 0 as a Console =====

image:../images/td-2x1-2nodes-kvm-schematic.png[Using a Console]

Consider a case where you have two systems, each with 1 GPU (more is possible too). 
You want to use display output port 0 of each of them as a system console. 
This case could happen on a workstation. Display Port 0 is typically used by 
the BIOS for output. When the kernel comes up, it also tends to use the same.
You would have connected the console output to some low-end device, e.g. a 
KVM dongle. Naturally you want to avoid using it as a display output. You'd 
use the parameter "remap_display_outputs" to get around this problem, as shown 
in the XML below.

----
<resourceGroup>
    <name>simple<name>
    <handler>tiled_display</handler>
    <handler_params>
        block_type="gpu";
        num_blocks=[2,1];           # We use 2*1 = 2 GPUs
        block_display_layout=[1,1]; # Each GPU drives one displays
        display_device="LP3065";    # Each display connected to an LP3065 monitor
        remap_display_outputs=[1];  # Map display output 0 to 1
    </handler_params>
    <resources>
        <reslist>
            <res><serverconfig><hostname>node1</hostname><serverconfig></res>
            <res><gpu><hostname>node1</hostname><index>0</index></gpu></res>
        </reslist>
        <reslist>
            <res><serverconfig><hostname>node2</hostname><serverconfig></res>
            <res><gpu><hostname>node2</hostname><index>1</index></gpu></res>
        </reslist>
    </resources>
</resourceGroup>
----

===== Using a 4x2 tiled display in multiple ways without recabling =====

Typically, the cabling from the graphics cards to the displays is fixed and cannot 
be changed. Consider the following contrived scenario, admittedly, a fairly 
non-standard scenario.  

image:../images/td-4x2-used-in-many-waysxml.png[Multiple setups on a 4x2 tiled display]

We have two nodes, each with two GPUs.  They are connected to a 4x2 tiled display,
 consisting of 8 HP LP3065 monitors.  The user wants to use the displays in 3 
different ways (more are possible, but these 4 are enough to demonstrate the principles).

1. left-2x2 : Left half using all GPUs on node1

2. right-2x2 : Right half using all GPUs on node2

3. center-2x2: 2x2 tiles in the center, but using all GPUs.

Note that the four displays on the right are wired in a slightly different way : 
output port 0 is connected to the monitor on the right, and ouput port 1 is 
connected to the monitor on the left.  Figure 14 shows the XML & the parameters 
needed to achieve each different scenario as well.

[NOTE]
============================================================================
With this arrangement, the whole 4x2 (called "full-4x2") display cannot be 
addressed together as a large display.  So using such layouts is not 
recommended.  However, the whole 4x2 can be used if the user is willing to 
modify application scripts to address this specific scenario.
============================================================================

